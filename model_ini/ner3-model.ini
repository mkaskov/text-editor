[strings]
checkpoints_directory = checkpoints/
[regex]
regex_string = \[_K_\]|\[_At_\]|\[\|\|\]|\[\/K\]|\[K\]|\[At\]|\[\/At\]|гост\s[\d.]+—?\-?[\d]+|ГОСТ\s[\d.]+—?\-?[\d]+|[а-яА-Я]+\/[а-яА-Я\d]+\.{1}[а-яА-Я\d]+\.{1}|[а-яА-Я]+\/\([^()]+\)|[^\s\d.,!?():;/\\|<>\"\'=–—\-+_*\xA0IV\[\]≥≤~”“_ₒ∙°··\x23«»]+\d?\/[^\s.,!?():;/\\|<>\"\'=–—\-+_*\xA0IV\[\]≥≤~”“_ₒ∙°··\x23«»]+|м{1,2}[\d⁰¹²³⁴⁵⁶⁷⁸⁹]|см[\d⁰¹²³⁴⁵⁶⁷⁸⁹]|дм[\d⁰¹²³⁴⁵⁶⁷⁸⁹]|[a-zA-Zа-яА-ЯёЁ]*[^\s\d.\\!?,:;()\"\'<>%«»±^…–—\-*=/+\xA0@·∙\[\]°ₒ”“·≥≤~_\x23]|[\d()\\!?\'\"<>%,:;±«»^…–—\-*=/+@·∙\[\]°ₒ”“·≥≤~_\x23]|\.{3}|\.{1}
[ints]
# vocabulary size 
in_vocab_size = 40000
out_vocab_size = 40000
# number of LSTM layers
num_layers = 3
# typical options : 128, 256, 512, 1024
layer_size = 512
# dataset size limit; typically none : no limit
max_train_data_size = 0
batch_size = 16
# steps per checkpoint
# 	Note : At a checkpoint, models parameters are saved, model is evaluated
#			and results are printed
steps_per_checkpoint = 500
window_size = 30
[floats]
learning_rate = 0.5
learning_rate_decay_factor = 0.9
max_gradient_norm = 5.0
[buckets]
buckets = [(32, 34)]
