[1mdiff --git a/app2web.py b/app2web.py[m
[1mindex 409572d..037c882 100644[m
[1m--- a/app2web.py[m
[1m+++ b/app2web.py[m
[36m@@ -92,40 +92,61 @@[m [mdef index():[m
     return "Decoding App"[m
 [m
 [m
[31m-@app.route('/web_decode')[m
[31m-def web_decode():[m
[31m-  with tf.Session() as sess:[m
[31m-    # Decode from input.[m
[31m-    sentence = "–ö—Ä–∞—Å–∫–∞"[m
[31m-    # Get token-ids for the input sentence.[m
[31m-    token_ids = data_utils.sentence_to_token_ids(sentence, in_vocab)[m
[31m-    # Which bucket does it belong to?[m
[31m-    bucket_id = min([b for b in xrange(len(_buckets))[m
[31m-                       if _buckets[b][0] > len(token_ids)])[m
[31m-    # Get a 1-element batch to feed the sentence to the model.[m
[31m-    encoder_inputs, decoder_inputs, target_weights = model.get_batch([m
[31m-          {bucket_id: [(token_ids, [])]}, bucket_id)[m
[31m-    # Get output logits for the sentence.[m
[31m-    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,[m
[31m-                                       target_weights, bucket_id, True)[m
[31m-    # This is a greedy decoder - outputs are just argmaxes of output_logits.[m
[31m-    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits][m
[31m-    # If there is an EOS symbol in outputs, cut them at that point.[m
[31m-    if data_utils.EOS_ID in outputs:[m
[31m-        outputs = outputs[:outputs.index(data_utils.EOS_ID)][m
[31m-    # Print out OUTPUT sentence corresponding to outputs.[m
[31m-    print((" ".join([rev_out_vocab[output] for output in outputs])).decode("utf-8"))[m
[31m-    print("> ", end="")[m
[31m-    sentence = ""[m
[31m-    return "web_decode_end"[m
[32m+[m[32m@app.route('/web_decode/<sentence>')[m
[32m+[m[32mdef web_decode(sentence):[m
[32m+[m[32m  # with tf.Session() as sess: //// –º–µ—Ç–æ–¥ try catch, –∑–¥–µ—Å—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è,—Ç–∞–∫ –∫–∞–∫ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ[m
[32m+[m[41m  [m
[32m+[m[32m  # —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö[m
[32m+[m[32m  # sentence = "–ö—Ä–∞—Å–∫–∞" # - old variant[m
[32m+[m[32m  # token_ids = data_utils.sentence_to_token_ids('–ö—Ä–∞—Å–∫–∞', in_vocab)[m
[32m+[m[32m  # print ("—ç—Ç–∞–ª–æ–Ω–Ω—ã–π –∞–π–¥–∏ –¥–ª—è –ö—Ä–∞—Å–∫–∞")[m
[32m+[m[32m  # print (token_ids)[m
[32m+[m[41m  [m
[32m+[m[32m  #—Å–Ω–∞—á–∞–ª–∞ –Ω–∞–¥–æ –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤ utf-8 –ø—Ä–∏—Ö–æ–¥—è—â–∏–π –∑–∞–ø—Ä–æ—Å, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–∞—Ä—å –∑–∞–ø–∏—Å–∞–Ω –≤ —ç—Ç–æ–º —Ñ–æ—Ä–º–∞—Ç–µ[m
[32m+[m[32m  sentence = sentence.encode('utf8')[m
[32m+[m[32m  # Get token-ids for the input sentence.[m
[32m+[m[32m  token_ids = data_utils.sentence_to_token_ids(sentence, in_vocab)[m
[32m+[m[32m  #–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏ –≤—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω—ã –≤–≤–µ–¥—ë–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞[m
[32m+[m[32m  print ("current token for")[m
[32m+[m[32m  print (sentence)[m
[32m+[m[32m  print (token_ids)[m
[32m+[m[41m  [m
[32m+[m[32m  # Which bucket does it belong to?[m
[32m+[m[32m  bucket_id = min([b for b in xrange(len(_buckets))[m
[32m+[m[32m                     if _buckets[b][0] > len(token_ids)])[m
[32m+[m[32m  # Get a 1-element batch to feed the sentence to the model.[m
[32m+[m[32m  encoder_inputs, decoder_inputs, target_weights = model.get_batch([m
[32m+[m[32m        {bucket_id: [(token_ids, [])]}, bucket_id)[m
[32m+[m[32m  # Get output logits for the sentence.[m
[32m+[m[32m  _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,[m
[32m+[m[32m                                     target_weights, bucket_id, True)[m
[32m+[m[32m  # This is a greedy decoder - outputs are just argmaxes of output_logits.[m
[32m+[m[32m  outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits][m
[32m+[m[32m  # If there is an EOS symbol in outputs, cut them at that point.[m
[32m+[m[32m  if data_utils.EOS_ID in outputs:[m
[32m+[m[32m      outputs = outputs[:outputs.index(data_utils.EOS_ID)][m
[32m+[m[32m  # Print out OUTPUT sentence corresponding to outputs.[m
[32m+[m[32m  retValue = (" ".join([rev_out_vocab[output] for output in outputs])).decode("utf-8")[m
[32m+[m[32m  print (sentence)[m
[32m+[m[32m  print(retValue)[m
[32m+[m[32m  print("> ", end="")[m
[32m+[m[32m  sentence = ""[m
[32m+[m[32m  # return "web_decode_end"[m
[32m+[m[32m  # –±—É–¥–µ–º –Ω–∞–ø—Ä–∏–º–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—Ä–∞–∑—É[m
[32m+[m[32m  return retValue[m
 [m
 [m
 def onstart():[m
[31m-    with tf.Session() as sess:[m
[31m-       # Create model and load parameters.[m
[31m-       print("load model")[m
[31m-       model = create_model(sess, True)[m
[31m-       model.batch_size = 1  # We decode one sentence at a time.[m
[32m+[m[32m  #–°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—Å–µ—Å—Å–∏—è —Ç–µ–Ω—Å–æ—Ä—Ñ–ª–æ—É –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è –º–æ–¥–µ–ª—å)[m
[32m+[m[32m  global sess[m
[32m+[m[32m  sess = tf.Session()[m
[32m+[m[32m  global model[m
[32m+[m[32m  # with tf.Session() as sess: //// –º–µ—Ç–æ–¥ try catch, –∑–¥–µ—Å—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è,—Ç–∞–∫ –∫–∞–∫ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ[m
[32m+[m
[32m+[m[32m  # Create model and load parameters.[m
[32m+[m[32m  print("load model")[m
[32m+[m[32m  model = create_model(sess, True)[m
[32m+[m[32m  model.batch_size = 1  # We decode one sentence at a time.[m
     [m
 [m
 if __name__ == "__main__":[m
